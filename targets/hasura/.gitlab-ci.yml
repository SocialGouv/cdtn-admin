#
#
#

üóÉ hasura:
  extends:
    - .autodevops_register_image
    - .cdtn_base_rules
  variables:
    CONTEXT: ./targets/hasura
    IMAGE_NAME: $CI_REGISTRY_IMAGE/hasura

#
#
#

# todo: restrict to branches ?
‚è™ Restore data:
  extends:
    - .base_deploy_kosko_stage
    - .cdtn_base_rules
  stage: .post
  cache:
    key:
      files:
        - .k8s/yarn.lock
      prefix: k8s
    paths:
      - .cache
      - .k8s/node_modules
  environment:
    name: ${AUTO_DEVOPS_PROD_ENVIRONMENT_NAME}
  needs:
    - job: Notify Starting Deployment
  allow_failure: true
  variables:
    KOSKO_GENERATE_ARGS: --env dev jobs/restore
  before_script:
    - export COMPONENT=$(cat <(echo "restore-${CI_COMMIT_REF_SLUG}" | cut -c1-24) <(echo "$CI_COMMIT_REF_SLUG" | sha1sum | cut -c1-6) | tr -d '\n')
    - echo "Use ${COMPONENT} as component label"
    - |
      if [[ "${CDTN_FORCE_RESTORE_DB}" == "true" ]]; then
        echo "Force restore database"
        kubectl -n cdtn-admin-secret delete all -l component=${COMPONENT} || true"
      fi
    # make the job fail if the component still exist
    - kubectl -n cdtn-admin-secret get all -l component=${COMPONENT} | grep -v 'No resources found.' && echo "Skip restore db" && exit 0

#
#
#

üöß Cleanup db cronjob (prod):
  extends:
    - .base_deploy_kosko_stage
    - .cdtn_base_rules
  rules:
    - if: "$PRODUCTION && $CI_COMMIT_TAG"
      when: always
  cache:
    key:
      files:
        - .k8s/yarn.lock
      prefix: k8s
    paths:
      - .cache
      - .k8s/node_modules
  environment:
    name: ${AUTO_DEVOPS_PROD_ENVIRONMENT_NAME}
  variables:
    KOSKO_GENERATE_ARGS: --env prod jobs/cleanup-db
